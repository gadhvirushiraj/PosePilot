{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('points_5.csv')\n",
    "data.drop('label',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 132)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM model class\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, sequence_length, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout=0.4, batch_first=True)\n",
    "\n",
    "        # # Attention layer\n",
    "        # self.attention = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=8, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size*sequence_length, 512)\n",
    "        self.fc2 = nn.Linear(512, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "        self.leaky_relu = nn.ReLU(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # # Initialize hidden state and cell state\n",
    "        # h0 = torch.randn(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        # c0 = torch.randn(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "    \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x)\n",
    "\n",
    "        # # Apply attention\n",
    "        # out, _ = self.attention(out, out, out)\n",
    "\n",
    "        # print(\"BEFORE FC\")\n",
    "        # print(out.shape)\n",
    "        # Decode the hidden state of the last time step :: out.shape = (batch_size, seq_length, hidden_size)\n",
    "        out = out.contiguous().view(out.size(0), -1)\n",
    "        # print(\"AFTER CONversion\")\n",
    "        # print(out.shape)\n",
    "        out = self.fc(out)\n",
    "        out = self.leaky_relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.leaky_relu(out)\n",
    "        out = self.fc3(out)\n",
    "        # print(\"AFTER FC\")\n",
    "        # print(out.shape)\n",
    "\n",
    "        # # Apply softmax activation along the sequence dimension\n",
    "        # out = torch.softmax(out, dim=1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 680  # Number of features\n",
    "hidden_size = 256 # Hidden layer size\n",
    "num_layers = 1  # Number of LSTM layers\n",
    "num_classes = 6  # Number of output classes\n",
    "learning_rate = 0.001  # Learning rate\n",
    "num_epochs = 10  # Number of epochs\n",
    "batch_size = 8 # Batch size\n",
    "sequence_length = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (lstm): LSTM(680, 256, batch_first=True, dropout=0.4)\n",
       "  (fc): Linear(in_features=2560, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=6, bias=True)\n",
       "  (leaky_relu): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNN(input_size, hidden_size, num_layers, sequence_length, num_classes).to(device) # Replace with your model class\n",
    "model.load_state_dict(torch.load('models/classify-model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featue_df shape:  (128, 68)\n",
      "shape of feature_df :  (128, 680)\n",
      "shape (128, 680)\n",
      "length of error list 128\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from classify_predict import predict\n",
    "\n",
    "\n",
    "\n",
    "output = predict(data,model)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
